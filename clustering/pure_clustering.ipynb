{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure clustering\n",
    "\n",
    "Test clustering algorithm that yields clusters exceeding a given \"purity\" level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import speasy as spz\n",
    "import sklearn\n",
    "import os\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import *\n",
    "from astropy import constants as Cst\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "# locally defined functions\n",
    "from coord_utils import *\n",
    "from plot_utils import *\n",
    "import amda_utils\n",
    "\n",
    "# default format for datetime objects\n",
    "date_fmt=\"%Y-%m-%dT%H:%M:%S\"\n",
    "# excess time for interpolation\n",
    "delta_t = timedelta(days=1)\n",
    "# interpolation freq\n",
    "freq = \"60S\"\n",
    "# ACE data timeshift\n",
    "shift_t = timedelta(seconds=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some labels for each of classes we wish to detect as well as the associated color mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magnetoshere regions\n",
    "region_labels = [\"SW\", \"MgtSth\", \"MgtSph\"]\n",
    "# color mapping used\n",
    "cmap = mpl.colors.ListedColormap(['r','g','b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time range of the data used for training and testing the model. The data provided by AMDA has variable sampling time. We interpolate each feature with a fixed sampling time. To avoid side effect we retrieve data from `start - delta_t` to `stop + delta_t`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start, stop = \"2007-03-01T00:00:00\", \"2008-03-01T00:00:00\"\n",
    "start, stop = \"2007-03-01T00:00:00\", \"2009-04-01T00:00:00\"\n",
    "start, stop = datetime.strptime(start, date_fmt), \\\n",
    "    datetime.strptime(stop, date_fmt)\n",
    "# used for naming data file\n",
    "start_str, stop_str = start.strftime(date_fmt),stop.strftime(date_fmt)\n",
    "# start and stop for getting data\n",
    "start_d, stop_d = start - delta_t, stop + delta_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ids of the parameters measured by the satellite:\n",
    "- `sat_b_id` : id of the satellite's magnetic field measurements (GSE) (nT)\n",
    "- `sat_n_id` : id of the satellite's ion density measurements \n",
    "- `sat_x_id` : id of the satellite's position data (GSE) (Re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Themis B - Prediction parameters\n",
    "sat_b_id = \"thb_bs\" # magnetic field mesurements in GSE coordinates\n",
    "sat_n_id = \"thb_n_i\" # plasma ion density\n",
    "sat_x_id = \"thb_xyz\" # target position in GSE coordinates\n",
    "\n",
    "# Themis A - prediction parameters\n",
    "#target_b = \"tha_bs\"\n",
    "#target_n = \"tha_n_i\"\n",
    "#target_x = \"tha_xyz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ids of the solar wind parameters : \n",
    "- `sw_b_id` : id of solar wind magnetic field measurements (GSE) (nT)\n",
    "- `sw_n_id` : id of solar wind ion density measurements \n",
    "- `sw_v_id` : id of solar wind speed (GSE) (km/s)\n",
    "- `sw_pdyn_id` : id of solar wind ram pressure (nP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solar wind measurements (AMDA/OMNI data)\n",
    "sw_b_id = \"omni1_imf\"\n",
    "sw_n_id = \"omni1_sw_n\"\n",
    "sw_v_id = \"omni1_sw_vgse\"\n",
    "sw_pdyn_id = \"omni1_sw_pdyn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the interpolated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = f\"parameter_data_{start_str}_{stop_str}.pkl\"\n",
    "\n",
    "parameter_ids = [sw_b_id,sw_n_id,sw_v_id,sw_pdyn_id] + \\\n",
    "                [sat_b_id, sat_n_id, sat_x_id]\n",
    "\n",
    "m = len(parameter_ids)\n",
    "parameter_shifts = {}#{\"imf\":shift_t, \"sw_n\":shift_t}\n",
    "\n",
    "t, interpolated_data, bounds = amda_utils.get_interpolated_data(parameter_ids, \n",
    "                                    start_d,\n",
    "                                    stop_d,\n",
    "                                    data_filename, \n",
    "                                    shifts=parameter_shifts,\n",
    "                                    freq=freq)\n",
    "\n",
    "print(f\"Interpolated data: \")\n",
    "print(f\"\\tstart : {bounds[0]}\")\n",
    "print(f\"\\tstop  : {bounds[1]}\")\n",
    "print(f\"\\tfreq  : {freq}\")\n",
    "print(f\"\\tdata shapes\")\n",
    "for k,v in interpolated_data.items():\n",
    "    print(f\"\\t\\t{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set asside the solar wind parameters for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solar wind data\n",
    "sw_n = interpolated_data[sw_n_id]\n",
    "sw_b = interpolated_data[sw_b_id]\n",
    "sw_v = interpolated_data[sw_v_id]\n",
    "sw_pdyn = interpolated_data[sw_pdyn_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the satellite measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Satellite measurements\n",
    "sat_n = interpolated_data[sat_n_id]\n",
    "sat_b = interpolated_data[sat_b_id]\n",
    "sat_xyz = interpolated_data[sat_x_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature analysis\n",
    "\n",
    "Let's check out the data. The two features we are interested in are:\n",
    "- `r_b` = $|b_{sat}|  /  |b_{sat}|$\n",
    "- `r_n` = $n_{sat}  /  n_{sw}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_B = np.linalg.norm(sw_b, axis=1) # solar wind magnetic field magnitude\n",
    "sat_B = np.linalg.norm(sat_b, axis=1) # satellite magnetic field magnitude\n",
    "\n",
    "r_b = sat_B / sw_B\n",
    "r_n = (sat_n / sw_n).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the feature array and compute the position bounds (useful for later plots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "x_full = np.vstack((r_n,r_b)).T\n",
    "\n",
    "\n",
    "# plot bounds for position in (X,Y) plane\n",
    "xmin,xmax = sat_xyz[:,0].min(), sat_xyz[:,0].max()\n",
    "ymin,ymax = sat_xyz[:,1].min(), sat_xyz[:,1].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `r_b` and `r_n` are concentrated around 0. Let's try transforming the data to make it easier to identify the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full = np.log(1+x_full)\n",
    "\n",
    "fig = plot_features(x_full, xlabel=\"log(1+r_n)\", ylabel=\"log(1+r_b)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the labels\n",
    "We compute the labels. Use the Shue1997 and Jerab2005 models for calculating the position of magnetopause and bow shock. Generating all the labels can take some time and we store the results in a local file to be reused on later executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "labels_data_filename = data_filename.split(\".\")[0]+\"_y.pkl\"\n",
    "if not os.path.exists(labels_data_filename):\n",
    "    y_full = get_regions_dyn(sat_xyz,\n",
    "                           sw_pdyn,\n",
    "                           sw_b[:,2],\n",
    "                           sw_n,\n",
    "                           np.linalg.norm(sw_v, axis=1),\n",
    "                           np.linalg.norm(sw_b, axis=1))\n",
    "    pkl.dump(y_full, open(labels_data_filename,\"wb\"))\n",
    "else:\n",
    "    print(f\"Loading labels from {labels_data_filename}\")\n",
    "    y_full = pkl.load(open(labels_data_filename, \"rb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalling the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "scaler = preprocessing.RobustScaler()\n",
    "scaler.fit(x_full)\n",
    "x_full = scaler.transform(x_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cluster_purity(y):\n",
    "    u,c = np.unique(y, return_counts=True)\n",
    "    return np.max(c)/np.sum(c), u[np.argmax(c)].astype(int)\n",
    "\n",
    "def my_clustering(x, y, cluster_f, n_clusters=3, tol=.9, max_n_clusters=10):\n",
    "    print(f\"my_clustering : {x.shape}, {y.shape}, {n_clusters}\")\n",
    "    if x.shape[0] <= n_clusters:\n",
    "        return None, None\n",
    "    if n_clusters > max_n_clusters:\n",
    "        return None, None\n",
    "    features, labels = np.array([]),np.array([])\n",
    "    rest_feat, rest_labels = np.array([]),np.array([])\n",
    "    # start clustering\n",
    "    clust = cluster_f(n_clusters=n_clusters)\n",
    "    y_pred = clust.fit_predict(x)\n",
    "    classes = set(y_pred)\n",
    "    print(\"\\tclasses : \", len(classes))\n",
    "    if len(classes) < n_clusters:\n",
    "        return None, None\n",
    "    recluster = []\n",
    "        \n",
    "    for cl in classes:\n",
    "        purity, cls = cluster_purity(y[y_pred==cl])\n",
    "        print(\"\\t\", purity, cls, np.sum(y_pred==cl))\n",
    "        if purity>=tol:\n",
    "            new_labels = np.ones(np.sum(y_pred==cl))*cls\n",
    "            new_feats = x[y_pred==cl]\n",
    "            features = _stack_arrays(features, new_feats)\n",
    "            labels = _stack_arrays(labels, new_labels)\n",
    "        else:\n",
    "            # set asside the rest of the data\n",
    "            new_labels = y[y_pred==cl]\n",
    "            new_feats = x[y_pred==cl]\n",
    "            recluster.append((new_feats, new_labels))\n",
    "            #f, l = my_clustering(new_feats, new_labels, cluster_f, n_clusters=n_clusters+1, tol=tol, max_n_clusters=max_n_clusters)\n",
    "            #rest_feat = _stack_arrays(rest_feat, new_feats)\n",
    "            #rest_labels = _stack_arrays(rest_labels, new_labels)\n",
    "            #features = _stack_arrays(features, f)\n",
    "            #labels = _stack_arrays(labels, l)\n",
    "    for nf,nl in recluster:\n",
    "        f, l = my_clustering(nf, nl, cluster_f, n_clusters=n_clusters+1, tol=tol, max_n_clusters=max_n_clusters)\n",
    "        features = _stack_arrays(features, f)\n",
    "        labels = _stack_arrays(labels, l)\n",
    "    #if rest_feat.size and rest_feat.shape[0]>n_clusters+1:\n",
    "    #    new_feats, new_labels = my_clustering(rest_feat, rest_labels, cluster_f, n_clusters=n_clusters+1, tol=tol, max_n_clusters=max_n_clusters)\n",
    "    #    features = _stack_arrays(features, new_feats)\n",
    "    #    labels = _stack_arrays(labels, new_labels)\n",
    "    return features, labels\n",
    "\n",
    "def my_clustering2(x, y, cluster_f, tol=.98, min_size=None, K=2):\n",
    "    # number of classes in the data\n",
    "    n_classes = len(set(y))\n",
    "    #print(set(y))\n",
    "    if min_size is None:\n",
    "        min_size=int(.1 * (x.shape[0]/(K*n_classes)))\n",
    "    if x.shape[0] < min_size:\n",
    "        return None, None\n",
    "    #print(f\"my_clustering2 : shape: {x.shape}, n_classes: {n_classes}, min_size: {min_size}\")\n",
    "    if x.shape[0] <= K * n_classes:\n",
    "        return None, None\n",
    "    if n_classes == 1: \n",
    "        return x, y\n",
    "\n",
    "    features, labels = np.array([]),np.array([])\n",
    "    rest_feat, rest_labels = np.array([]),np.array([])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # start clustering\n",
    "    clust = cluster_f(n_clusters=K * n_classes)\n",
    "    y_pred = clust.fit_predict(x)\n",
    "    classes = set(y_pred)\n",
    "    \n",
    "    #print(\"\\tclasses : \", len(classes))\n",
    "    if len(classes) < n_classes:\n",
    "        return None, None\n",
    "    recluster = []\n",
    "        \n",
    "    for cl in classes:\n",
    "        purity, cls = cluster_purity(y[y_pred==cl])\n",
    "        #print(\"\\t\", purity, cls, np.sum(y_pred==cl))\n",
    "        if purity>=tol:\n",
    "            new_labels = (np.ones(np.sum(y_pred==cl))*cls).astype(int)\n",
    "            new_feats = x[y_pred==cl]\n",
    "            features = _stack_arrays(features, new_feats)\n",
    "            labels = _stack_arrays(labels, new_labels).astype(int)\n",
    "        else:\n",
    "            # set asside the rest of the data\n",
    "            new_labels = y[y_pred==cl].astype(int)\n",
    "            new_feats = x[y_pred==cl]\n",
    "            \n",
    "            if new_labels.shape[0] >= min_size:\n",
    "                recluster.append((new_feats, new_labels))\n",
    "            \n",
    "    for nf, nl in recluster:\n",
    "        f, l = my_clustering2(nf, nl, cluster_f, tol=tol, min_size=min_size)\n",
    "        features = _stack_arrays(features, f)\n",
    "        labels = _stack_arrays(labels, l).astype(int)\n",
    "\n",
    "    return features, labels  \n",
    "\n",
    "from sklearn.cluster import KMeans, Birch\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from cluster_utils import _stack_arrays\n",
    "\n",
    "m = int(x_full.shape[0]/100)\n",
    "rindx = np.random.choice(range(x_full.shape[0]), size=5000)\n",
    "\n",
    "f, l= my_clustering2(x_full[rindx], y_full[rindx], SpectralClustering)\n",
    "\n",
    "plt.hist(y_full[rindx], bins=100, density=True)\n",
    "\n",
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].scatter(f[:,0], f[:,1], c=l, alpha=.3, marker=\".\")\n",
    "axes[1].hist(l, bins=100, density=True)\n",
    "\n",
    "\n",
    "f, l= my_clustering2(x_full[rindx], y_full[rindx], SpectralClustering, K=3, tol=.9)\n",
    "\n",
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].scatter(f[:,0], f[:,1], c=l, alpha=.3, marker=\".\")\n",
    "axes[1].hist(l, bins=100, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = []\n",
    "tols = np.linspace(0.9, 0.99, 10)\n",
    "for p in tols:\n",
    "    print(p)\n",
    "    f, l= my_clustering2(x_full[rindx], y_full[rindx], SpectralClustering, K=2, tol=p)\n",
    "    n_classes.append(len(set(l)))\n",
    "    \n",
    "\n",
    "plt.plot(tols, n_classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speasy_use_case",
   "language": "python",
   "name": "speasy_use_case"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
